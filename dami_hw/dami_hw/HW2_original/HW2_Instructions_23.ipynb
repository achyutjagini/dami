{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Unsupervised Learning - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this homework you will work with Unsupervised learning and cluster a real-world dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Student information**\n",
    "Please provide your information for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'Your su account (e.g., pang1234)'\n",
    "STUD_NAME = 'First and Last name'\n",
    "STUD_EMAIL = 'student@stud.dsv.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grading: \n",
    "\n",
    "Total points: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTLINE\n",
    "## Total points: 5\n",
    "\n",
    "1. Euclidean - Manhattan , **points: 0.5**\n",
    "2. Purity , **points: 0.6**\n",
    "3. Read and preprocess the wine dataset, **points: 0.2**\n",
    "4. Elbow method, **points: 0.7**\n",
    "5. Run K-means, and Agglomerative clustering on the wine dataset, **points: 0.4**\n",
    "6. Evaluation metrics, **points: 0.3**\n",
    "7. Plotting, **points: 0.3**\n",
    "6. Implementation of K-medians, **points: 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values (we refer to it as 'change this') to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please only do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs,  **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset that you will use in this assignment is the Wine Dataset which you will download using sklearns dataset module (instructions are provided below in Task3):\n",
    "\n",
    "Information can be found in the [link](https://archive.ics.uci.edu/ml/datasets/wine)\n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "    - Alcohol\n",
    "    - Malic acid\n",
    "    - Ash\n",
    "    - Alcalinity of ash\n",
    "    - Magnesium\n",
    "    - Total phenols\n",
    "    - Flavanoids\n",
    "    - Nonflavanoid phenols\n",
    "    - Proanthocyanins\n",
    "    - Color intensity\n",
    "    - Hue\n",
    "    - OD280/OD315 of diluted wines\n",
    "    - Proline\n",
    "\n",
    "All attributes are numerical. The are no missing values. \n",
    "\n",
    "The target/class is the 3 different types of wines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import here all the libraries needed for this assignment\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "RSEED = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Euclidean-Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:`\n",
    "\n",
    "###  `In the function named calculate_distances: `\n",
    "###  `- compute the Euclidean distance between any two given vectors (numpy arrays)`\n",
    "\n",
    "The euclidean distance:\n",
    "$d_e = \\sqrt{\\sum_{i=1}^{n}({a_i-b_i})^2}$\n",
    "\n",
    "###  `- compute the Manhattan distance between any two given vectors (numpy arrays)`\n",
    "\n",
    "The manhattan distance: \n",
    "$d_m = \\sum_{i=1}^{n}{\\lvert a_i-b_i \\rvert}$ \n",
    "\n",
    "**Important**: You should implement this yourself from scratch (using numpy functions). If you call a method (any predefined method) to calculate the distances the task will get no points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(vector1, vector2, name_of_distance_metric):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            vector1: numpy array \n",
    "            vector2: numpy array \n",
    "            name_of_distance_metric: string taking values: 'euclidean' or 'manhattan'\n",
    "          \n",
    "    Output:\n",
    "            distance: the value of the distance that you calculated\n",
    "    \n",
    "\n",
    "    step 1: if the name of the distance metric is \"euclidean\" then store in the output variable the result of the euclidean distance between vector1 and vector2\n",
    "    step 2: if the name of the distance metric is \"manhattan\" then store in the output variable the result of the manhattan distance between vector1 and vector2\n",
    "        \n",
    "    Note: implement this yourself from scratch! if you want to make sure that the calculations are correct, cross check it by calculating the manhattan/euclidean distances by\n",
    "    hand or by calling a method that does these calculations, to see what's the correct result. Of course you should not include these checks in the function \n",
    "    but your numpy implementation!\n",
    "    \"\"\"\n",
    "    #write your code here\n",
    "    distance = None # change this\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test your function here using these two arrays (do not change them!).\n",
    "# do not delete this!\n",
    "v1 = np.array((5, 8, 9, 4, 34))\n",
    "v2 = np.array((1, 5, 89, 7, 2))\n",
    "euclidean = calculate_distances(v1,v2, \"euclidean\")\n",
    "manhattan = calculate_distances(v1,v2, \"manhattan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Purity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:  In the function named purity calculate the purity metric for any given clustering result. `\n",
    "\n",
    "Read more about purity [here](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html) \n",
    "\n",
    "\n",
    "You will use the purity function later on the evaluation of the clustering results of the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "To calculate Purity first create a contigency matrix (you can call the sklearn's version of the contigency matrix [link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cluster.contingency_matrix.html#sklearn.metrics.cluster.contingency_matrix)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of how a contigency matrix works: \n",
    "# suppose that this array holds your ground truth labels\n",
    "true = ['a', 'b', 'a', 'b', 'a'] # so we have two groupings that belong to 'a' and 'b'. Sample 0 belongs to group 'a', sample 1 belongs to group 'b' and so on. \n",
    "# suppose that we did some clustering and that this array holds your predicted labels from a clustering algorithm:\n",
    "predicted = [1, 1, 1, 0, 1] # so samples 0 to 2 and sample 4 belong to cluster 1, sample 3 to cluster 0 according to some clustering algorithm\n",
    "\n",
    "# let's calculate the contigency matrix for the above \n",
    "metrics.cluster.contingency_matrix(['a', 'b', 'a', 'b', 'a'], [1, 1, 1, 0, 1])\n",
    "\n",
    "# The first row of output array indicates that there are 3 samples whose true cluster is “a”. \n",
    "# Of them, None are predicted in cluster 0, and 3 are predicted in cluster 1. \n",
    "# And the second row indicates that there are 2 samples whose true cluster is “b”. Of them, 1 is predicted in cluster 0, and 1 is predicted in cluster 1.\n",
    "# If you'd like you could use the result of the contigency matrix to calculate purity! check the link above or the lecture material to see how purity is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "        y_true: numpy array, the true labels of your dataset \n",
    "        y_pred: numpy array, the labels predicted by the algorithm\n",
    "    Output:\n",
    "        purity: the resulting value for purity \n",
    "\n",
    "\n",
    "    step 1: create the contigency matrix for the true and predicted labels \n",
    "    step 2: find the max value of correctly assigned samples in the cluster (each cluster is assigned to the class which is most frequent in the cluster)\n",
    "    step 3: sum the correcly assigned samples and divide by the sum of points in the contigency matrix\n",
    "    \"\"\"\n",
    "    # write your code here \n",
    "\n",
    "    purity = None # change this\n",
    " \n",
    "    return purity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test your function here using these two arrays (do not change them!).\n",
    "# do not delete this!\n",
    "ground_truth = pd.Series([1,0,2,1,2,1,0,1,1,0,0])\n",
    "predicted_labels = pd.Series([1,0,2,1,2,1,0,1,1,2,0])\n",
    "\n",
    "\n",
    "purity(ground_truth, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read and preprocess the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the wine dataset from sklearn \n",
    "wine = datasets.load_wine()\n",
    "\n",
    "data = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "\n",
    "# the column class is your target/class. The seperation of samples to the 3 different wine cultivators\n",
    "data['class'] = wine.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to see information about the dataset \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,:-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Standardize all the numerical features. You can use the StandardScaler from sklearn`\n",
    "**Important**: Remember that the class/target should not be standardized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Initialize a StandardScaler object\n",
    "Step 2: fit_transform the values of the numerical features (not the class!)\n",
    "Step 3: Transform the standardized returned array into a dataframe called **data_standardized** with the corresponding column names\n",
    "\"\"\"\n",
    "# write your code here\n",
    "\n",
    "data_standardized = pd.DataFrame([]) # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this cell!\n",
    "data_standardized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elbow Method\n",
    "\n",
    "In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The elbow method plots the value of the cost function (inertia) produced by different values for the number of clusters. The elbow of the curve indicates the point that we should stop dividing the data into further clusters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task:` \n",
    "\n",
    "`In the function named elbow_method: - Implement the elbow method for Kmeans. - Plot the elbow`\n",
    "\n",
    "`Following that:- store in the variable called number_of_clusters the best number of clusters according to the elbow method for Kmeans`\n",
    "\n",
    "About the elbow plot:\n",
    "\n",
    "* x-axis: the number of clusters\n",
    "\n",
    "* y-axis: inertia/distortion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(X, max_range_for_elbow, rseed = RSEED):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X: dataframe or numpy array, the dataset to be clustered, \n",
    "        max_range_for_elbow: int, the max number of clusters you want the elbow method to run. \n",
    "\n",
    "    **1st way**:\n",
    "    step1: \n",
    "        create an empty list where you will store the inertia \n",
    "    step2:\n",
    "        Create a sequence of numbers from 1 to max_range_for_elbow and store in a variable called K, these will be your sequence of numbers for the various numbers of clusters\n",
    "    step3: \n",
    "        in a for loop that goes through the values of K one by one, run Kmeans for each of these values in the range with random_state=RSTATE\n",
    "    step4: \n",
    "        inside the for loop, calculate the inertia for the clustering that you currently ran and append in the list from step 1. \n",
    "        The inertia is an attribute of the Kmeans object\n",
    "    step5: \n",
    "        Outside the for loop, plot the resulting elbow, in x-axis: K (sequence of numbers from 1 to max_range_for_elbow+1) and in y-axis: the inertia for the respective values of K\n",
    "\n",
    "    **2nd way**:\n",
    "        Another way to implement the elbow method is to use the KElbowVisualizer from yellowbricks! \n",
    "        This link will help you get an insight on how you could use it: https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
    "    \n",
    "    **Note**\n",
    "        The above steps/different ways of implementing the elbow method are just indicative. Please implement this as you want as long as you plot the required elbow plot and\n",
    "        store in the variable called number_of_clusters the best number of clusters according to the elbow method for Kmeans.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "# calling the elbow method here\n",
    "elbow_method(data_standardized,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best number of clusters according to the elbow method for Kmeans? Find the elbow!\n",
    "# store the result here and use it for the rest of the assignment as input to the algorithms. the assignment should be an integer number. \n",
    "# do not delete this!\n",
    "\n",
    "number_of_clusters = None # change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Run K-means, and Agglomerative Clustering on the wine dataset\n",
    "\n",
    "\n",
    "### `Task: Run K-means, Agglomerative with ward distance, Agglomerative with complete distance (use sklearn) and store the clustering labels in the variables indicated below for the respective algorithms. If you would like some help for this part you could either check the sklearn user-guide for the algorithms asked or check the examples of lab2.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-means (from sklearn) with the optimal number of clusters that you found above, and random_state=RSEED   \n",
    "# store the cluster memberships in kmeans_labels \n",
    "\n",
    "# write your code here \n",
    "\n",
    "\n",
    "kmeans_labels = None # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict = {}\n",
    "\n",
    "labels_dict[\"kmeans\"] = kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Agglomerative clustering (from sklearn) with ward distance and the optimal number of clusters that you found above\n",
    "# store the cluster memberships in ward_labels \n",
    "\n",
    "# write your code here \n",
    "\n",
    "\n",
    "ward_labels = None # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict[\"ward\"] = ward_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Agglomerative clustering (from sklearn) with complete distance and the optimal number of clusters that you found above\n",
    "# store the cluster memberships in complete_labels \n",
    "\n",
    "# write your code here \n",
    "\n",
    "\n",
    "complete_labels  = None # change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "labels_dict[\"complete\"] = complete_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: Write a function that calculates the silhouette score (from sklearn) and the purity (use the function from task 2) for any given clustering.`\n",
    "\n",
    "Specifically:\n",
    "\n",
    "- calculate silhouette score for any given clustering with the given metric and rseed = RSEED (use sklearn).\n",
    "- call the purity function that you defined above and calculate purity for any given clustering.\n",
    "- return the values for silhoutte score and purity in this order (s_s, pu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(X, labels_pred, labels_true,  metric, rseed = RSEED):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        X: array-like, the dataframe used for clustering\n",
    "        labels_pred: numpy array, the labels predicted by the algorithm\n",
    "        labels_true: numpy array, the target/class\n",
    "        metric: string, the metric to be used for silhouette score, example: 'euclidean'          \n",
    "\n",
    "    Output:\n",
    "        s_s: the value as calculated by silhouette\n",
    "        pu: the value as calculated by the purity function\n",
    "\n",
    "    # Note: if you could not implement purity in task 2, return the s_s value as calculated and pu=0\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    s_s = 0 # change this\n",
    "    pu = 0 # change this\n",
    "\n",
    "    return s_s, pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to check your evaluation metrics for all the clusterings that you ran in task 5.  \n",
    "# do not delete this!\n",
    "\n",
    "\n",
    "for keys, values in labels_dict.items():\n",
    "    print(\"Clustering Algorithm: \", keys)\n",
    "    s_s, pu = evaluation_metrics(data_standardized, values, wine.target, 'euclidean', rseed = RSEED)\n",
    "    if pu == None:\n",
    "        print(\"Silhouette Score: \", np.round(s_s, decimals=3) , \"Purity: \", pu)\n",
    "    else:\n",
    "        print(\"Silhouette Score: \", np.round(s_s, decimals=3) , \"Purity: \", np.round(pu, decimals=3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting\n",
    "\n",
    "### `Task: Reduce the dimensionality of the dataset to 2 principal components. Create 1 figure with 4 plots where you will plot the 2 principal components with colors respective to the class (plot/column 1) and the clustering labels of the 3 algorithms (k-means, aggromerative with two different distances) that you ran in task 5 (plot/columns 2-4). `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "step 1: Reduce the dimensionality of the standardized dataset in 2 Principal Components, with Principal Component analysis.  store the reduced dataset in **pca_2d**\n",
    "step 2: Create a sublot object with 1 row and 4 columns\n",
    "step 3: Plot the two principal components (use a scatterplot) with colors respective to the true labels\n",
    "step 4-7: Plot the two principal components (use a scatterplot) with colors respective to the clustering labels of the algorithms that we ran above\n",
    "you can check Lab2-Unsuperised learning for a similar example\n",
    "don't forget to put the titles on the respective plots!\n",
    "\"\"\"\n",
    "\n",
    "pca_2d = np.zeros((10,10)) #change this\n",
    "\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. K-medians "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Task: In the function called Kmedians implement Kmedians.`\n",
    "\n",
    "Detailed instructions can be found inside the method. \n",
    "\n",
    "**K-medians:** Instead of taking the mean value of the object in a cluster as a reference point, medians are used (**L1-norm** as the distance measure)\n",
    "\n",
    "\n",
    "The K-medians clustering algorithm:\n",
    "\n",
    "\n",
    "-Select K points as the initial representative objects (choose the initial random points from the dataset samples)\n",
    "\n",
    "-Repeat\n",
    "\n",
    "* Assign every point to its nearest **median**\n",
    "\n",
    "* Re-compute the median of each individual feature\n",
    "    \n",
    "Until convergence is satisfied   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmedians(X, n_clusters, rseed=RSEED):\n",
    "    \"\"\"\n",
    "    <Note> The implementation is very similar to the Kmeans implementation we saw in the lab. Follow this implementation, and use the documention of numpy \n",
    "    to see the differences between the mean and median functions, and how to use the median in this case. \n",
    "\n",
    "    Input:\n",
    "        X: numpy array, the dataset to be clustered, \n",
    "        n_clusters: the number of clusters you want your algorithm to run with, \n",
    "        RSEED: the random seed\n",
    "    Output:\n",
    "        centers: numpy array, the cluster centers of the algorithm you are currently running, \n",
    "        labels: numpy array, the clustering labels\n",
    "\n",
    "    step 1: Generate a random number using np.random.RandomState and with the random seed provided in parameters\n",
    "    step 2: Get the n_clusters first elements from a randomly permulated sequence with the length equal to the number of rows in X using the random number from step 1\n",
    "    step 3: Retrieve from X the elements with the indices found above and make those as our first random centers\n",
    "    step 4: assign each point to the nearest centers using the L1-norm as the distance measure (manhattan) and store it in a variable called labels \n",
    "    step 5: Update the centroids of each cluster using the median and store in a variable called new_centers\n",
    "    step 6: while the new_centers (from step 5) are not equal to the previously created centers, repeat the process in step 4 and 5.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # write your code here \n",
    "    \n",
    "    \n",
    "    return np.zeros((4)), np.zeros(10) # change this zero np arrays to the proper outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do not delete this!\n",
    "\n",
    "centers, labels = Kmedians(data_standardized.values, number_of_clusters)\n",
    "\n",
    "s_s, pu = evaluation_metrics(data_standardized.values, labels, wine.target, 'euclidean', rseed = RSEED)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=pca_2d[:, 0],y=pca_2d[:, 1], hue=labels)\n",
    "if pu == None:\n",
    "    plt.title(\"Wine Kmedians, PU: {},  SS: {}\".format(pu, np.round(s_s, decimals=3)))  \n",
    "else:\n",
    "    plt.title(\"Wine Kmedians, PU: {},  SS: {}\".format(np.round(pu, decimals=3), np.round(s_s, decimals=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE: Each function you make will be graded, so it is important to strictly follow input and output instructions stated in the skeleton code. Some of the cells have already some variables that are filled with None values or empty dataframes, you should change those nan/empty values (we refer to it as 'change this') to what is asked in the tasks (we only stored the empty values so the whole notebook can run error free). You should not delete any of the given cells as they will help us grade the assignment. Some cells ask you to uncomment some comments, please only do so if you have solved the respective task. When you are finished with implementing all the tasks, clear all outputs,  **restart the kernel**, run all cells again (make sure there is no error) and submit! Make sure that the results and figures asked are visible for us to grade. ` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
